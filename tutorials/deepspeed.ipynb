{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7075fea6",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ddb54",
   "metadata": {},
   "source": [
    "- https://github.com/microsoft/DeepSpeed\n",
    "    - https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/\n",
    "- pip 安装之后，可以通过 `ds_report` 命令查看环境配置信息；\n",
    "- bag of tricks\n",
    "    - $O(n^2)$（$n$ 表示序列长度） => sparse attention；\n",
    "        - 10x longer seq，up to 6x faster；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f6ac8d",
   "metadata": {},
   "source": [
    "## ZeRO: Zero Redundancy Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef825023",
   "metadata": {},
   "source": [
    "## 使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe743d9",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260c134",
   "metadata": {},
   "source": [
    "- inference-tutorial\n",
    "    - https://github.com/microsoft/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004b6eae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T14:10:21.585416Z",
     "start_time": "2023-12-14T14:10:18.133337Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb232dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T14:10:23.215039Z",
     "start_time": "2023-12-14T14:10:22.674084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-14 22:10:22,675] [INFO] [comm.py:606:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2023-12-14 22:10:23,210] [INFO] [comm.py:656:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=169.254.3.1, master_port=29500\n",
      "[2023-12-14 22:10:23,211] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To avoid warnings about parallelism in tokenizers\n",
    "\n",
    "# distributed setup\n",
    "local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n",
    "torch.cuda.set_device(local_rank)\n",
    "deepspeed.init_distributed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3a66c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:43:18.663465Z",
     "start_time": "2023-12-15T14:43:18.660168Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3169b104",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:50:36.643703Z",
     "start_time": "2023-12-15T14:43:19.833705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f1e9b742a141cabe2662a42f5faa2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a1074251c64fd2ab4d4b89c34dd56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/10.7G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89ddd13bf0c438bb5e480c1a6c0d887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e448e31dd6a0474db432ffdb176a91d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479e43d753f64ccfbfbc75aa91ba386b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8072ae5e5c4d97bb418fd502f39f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_rank = int(os.getenv('LOCAL_RANK', '0'))\n",
    "world_size = int(os.getenv('WORLD_SIZE', '1'))\n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B',\n",
    "                     device=local_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975191dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:51:08.349738Z",
     "start_time": "2023-12-15T14:50:40.371348Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-15 22:50:40,372] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown\n",
      "[2023-12-15 22:50:40,376] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2023-12-15 22:50:40,377] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/whaow/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "Creating extension directory /home/whaow/.cache/torch_extensions/py310_cu117/transformer_inference...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/whaow/.cache/torch_extensions/py310_cu117/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/9] /usr/local/cuda-11.7/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-11.7/include -isystem /home/whaow/anaconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/relu.cu -o relu.cuda.o \n",
      "[2/9] /usr/local/cuda-11.7/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-11.7/include -isystem /home/whaow/anaconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu -o dequantize.cuda.o \n",
      "[3/9] /usr/local/cuda-11.7/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-11.7/include -isystem /home/whaow/anaconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu -o apply_rotary_pos_emb.cuda.o \n",
      "[4/9] /usr/local/cuda-11.7/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-11.7/include -isystem /home/whaow/anaconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu -o transform.cuda.o \n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(63): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(103): warning #177-D: variable \"half_dim\" was declared but never referenced\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int) [with T=__nv_bfloat16]\" \n",
      "(244): here\n",
      "\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(120): warning #177-D: variable \"vals_half\" was declared but never referenced\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int) [with T=__nv_bfloat16]\" \n",
      "(244): here\n",
      "\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(121): warning #177-D: variable \"output_half\" was declared but never referenced\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int) [with T=__nv_bfloat16]\" \n",
      "(244): here\n",
      "\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(138): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int) [with T=__nv_bfloat16]\" \n",
      "(244): here\n",
      "\n",
      "[5/9] /usr/local/cuda-11.7/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-11.7/include -isystem /home/whaow/anaconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu -o gelu.cuda.o \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/9] /usr/local/cuda-11.7/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-11.7/include -isystem /home/whaow/anaconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu -o softmax.cuda.o \n",
      "[7/9] /usr/local/cuda-11.7/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-11.7/include -isystem /home/whaow/anaconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -c /home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/layer_norm.cu -o layer_norm.cuda.o \n",
      "[8/9] c++ -MMD -MF pt_binding.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/TH -isystem /home/whaow/anaconda3/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-11.7/include -isystem /home/whaow/anaconda3/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++14 -g -Wno-reorder -c /home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp -o pt_binding.o \n",
      "In file included from /home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:10:\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes/inference_context.h: In member function ‘void InferenceContext::GenWorkSpace(const unsigned int&, const unsigned int&, const size_t&, const size_t&, const size_t&, const unsigned int&, const bool&, const size_t&, const unsigned int&, unsigned int, unsigned int)’:\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes/inference_context.h:139:52: warning: format ‘%d’ expects argument of type ‘int’, but argument 2 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "  139 |                 \"Allocatable workspace available (%d tokens) is less than minimum requested \"\n",
      "      |                                                   ~^\n",
      "      |                                                    |\n",
      "      |                                                    int\n",
      "      |                                                   %ld\n",
      "  140 |                 \"workspace (%d tokens)\\n\",\n",
      "  141 |                 _max_seq_len,\n",
      "      |                 ~~~~~~~~~~~~                        \n",
      "      |                 |\n",
      "      |                 size_t {aka long unsigned int}\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&) [with T = float]’:\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1809:5:   required from here\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  536 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  537 |                                       k * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  545 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  546 |                           k * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&) [with T = __half]’:\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1810:5:   required from here\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  536 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  537 |                                       k * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  545 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  546 |                           k * InferenceContext::Instance().GetMaxTokenLenght(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/9] c++ pt_binding.o gelu.cuda.o relu.cuda.o layer_norm.cuda.o softmax.cuda.o dequantize.cuda.o apply_rotary_pos_emb.cuda.o transform.cuda.o -shared -lcurand -L/home/whaow/anaconda3/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-11.7/lib64 -lcudart -o transformer_inference.so\n",
      "Time to load transformer_inference op: 25.59630537033081 seconds\n",
      "[2023-12-15 22:51:06,713] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 2560, 'intermediate_size': 10240, 'heads': 20, 'num_hidden_layers': -1, 'fp16': False, 'pre_layer_norm': True, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'q_int8': False, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 256, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False}\n",
      "Time to load transformer_inference op: 0.0027832984924316406 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module transformer_inference...\n",
      "Using /home/whaow/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Free memory : 2.528137 (GigaBytes)  \n",
      "Total memory: 23.649719 (GigaBytes)  \n",
      "Requested memory: 0.800781 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 1024 \n",
      "WorkSpace: 0x7f5500000000 \n",
      "------------------------------------------------------\n",
      "[{'generated_text': 'DeepSpeed is not just a name for your hard drive. It’s a name that tells your computer about your system configuration and how it can improve storage performance. You can run one of these programs from the command line or right in the Task'}]\n"
     ]
    }
   ],
   "source": [
    "generator.model = deepspeed.init_inference(generator.model,\n",
    "                                           mp_size=world_size,\n",
    "                                           dtype=torch.float,\n",
    "                                           replace_with_kernel_inject=True)\n",
    "\n",
    "string = generator(\"DeepSpeed is\", do_sample=True, min_length=50)\n",
    "if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dbfbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
